{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"reduce_mrmr_instances\": pd.read_csv('./data/df_reduce_mrmr_instances.csv'),\n",
    "    \"reduce_mrmr_instances_hard\": pd.read_csv('./data/df_reduce_mrmr_instances_hard.csv'),\n",
    "    \"reduce_RFC_instances\": pd.read_csv('./data/df_reduce_RFC_instances.csv'),\n",
    "    \"reduce_RFC_instances_hard\": pd.read_csv('./data/df_reduce_RFC_instances_hard.csv'),\n",
    "    \"reduce_mrmr_instances_GLVQ\": pd.read_csv('./data/df_reduce_mrmr_instances_GLVQ.csv'),\n",
    "    \"reduce_RFC_instances_GLVQ\": pd.read_csv('./data/df_reduce_RFC_instances_GLVQ.csv'),\n",
    "    # \"train_data\": pd.read_csv('./data/train_data.csv')\n",
    "    \"test_data\" : pd.read_csv('./data/test_data.csv')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joses\\AppData\\Local\\Temp\\ipykernel_1360\\1107825650.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_reduced['Amount'] = scaler.fit_transform(x_reduced[['Amount']])\n",
      "C:\\Users\\joses\\AppData\\Local\\Temp\\ipykernel_1360\\1107825650.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_reduced['Time'] = scaler.fit_transform(x_reduced[['Time']])\n"
     ]
    }
   ],
   "source": [
    "# Seleccionamos los datos de prueba del conjunto de datos\n",
    "data_prueba = datasets['test_data']\n",
    "\n",
    "# Separar las características (X) de la variable objetivo (y)\n",
    "x = data_prueba.drop(columns=['Class'])\n",
    "y_test_total = data_prueba['Class']\n",
    "\n",
    "# Filtramos las columnas relevantes según mRMR\n",
    "columns_to_keep = ['V17', 'Time', 'Amount', 'V25', 'V20', 'V7', 'V13', 'V22', 'V19', 'V23']\n",
    "x_reduced = x[columns_to_keep]\n",
    "\n",
    "# Crear el scaler para normalizar los datos\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalizamos las columnas seleccionadas\n",
    "x_reduced['Amount'] = scaler.fit_transform(x_reduced[['Amount']])\n",
    "x_reduced['Time'] = scaler.fit_transform(x_reduced[['Time']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regresion con mrMr clusterCentroid_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n",
      "Mejores parámetros: {'C': 0.1, 'class_weight': 'balanced', 'fit_intercept': True, 'max_iter': 100, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.0001}\n",
      "Mejor puntuación de validación cruzada: 0.8377987567249356\n"
     ]
    }
   ],
   "source": [
    "df_reduce_mrmr_instances = datasets[\"reduce_mrmr_instances\"]\n",
    "X = df_reduce_mrmr_instances.drop(columns=['Class'])\n",
    "y = df_reduce_mrmr_instances['Class']\n",
    "\n",
    "# Dividir en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = [\n",
    "    # Configuración para 'elasticnet'\n",
    "    {\n",
    "        'penalty': ['elasticnet'],\n",
    "        'solver': ['saga'],  # Solo 'saga' es compatible con elasticnet\n",
    "        'C': [0.1, 1, 10],\n",
    "        'l1_ratio': [0.5],\n",
    "        'tol': [0.0001, 0.001],\n",
    "        'max_iter': [100, 200],\n",
    "        'class_weight': [None, 'balanced'],\n",
    "        'fit_intercept': [True],\n",
    "    },\n",
    "    # Configuración para 'l1' y 'l2'\n",
    "    {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga'],  # Solvers compatibles con l1 y l2\n",
    "        'C': [0.1, 1, 10],\n",
    "        'tol': [0.0001, 0.001],\n",
    "        'max_iter': [100, 200],\n",
    "        'class_weight': [None, 'balanced'],\n",
    "        'fit_intercept': [True],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Configurar el modelo de regresión logística\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Usar GridSearchCV para buscar la mejor combinación de parámetros\n",
    "# Dada la naturaleza desbalanceada de los datos\n",
    "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores parámetros y el mejor puntaje\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Mejores parámetros:\", best_params)\n",
    "print(\"Mejor puntuación de validación cruzada:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutamos para el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de Confusión:\n",
      "[[27661   771]\n",
      " [   14    35]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Correctas       1.00      0.97      0.99     28432\n",
      "Fraudulentas       0.04      0.71      0.08        49\n",
      "\n",
      "    accuracy                           0.97     28481\n",
      "   macro avg       0.52      0.84      0.53     28481\n",
      "weighted avg       1.00      0.97      0.98     28481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(x_reduced)\n",
    "test_accuracy = accuracy_score(y_test_total, y_pred)\n",
    "\n",
    "# Calcular la matriz de confusión y el reporte de clasificación\n",
    "conf_matrix = confusion_matrix(y_test_total, y_pred)\n",
    "report = classification_report(y_test_total, y_pred, target_names=['Correctas', 'Fraudulentas'])\n",
    "\n",
    "# Mostrar la matriz de confusión y el reporte\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regresion con mrMr clusterCentroid_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n",
      "Mejores parámetros: {'C': 0.1, 'class_weight': None, 'fit_intercept': True, 'max_iter': 100, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
      "Mejor puntuación de validación cruzada: 0.8559584457097194\n"
     ]
    }
   ],
   "source": [
    "df_reduce_mrmr_instances_hard = datasets[\"reduce_mrmr_instances_hard\"]\n",
    "X = df_reduce_mrmr_instances_hard.drop(columns=['Class'])\n",
    "y = df_reduce_mrmr_instances_hard['Class']\n",
    "\n",
    "# Dividir en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = [\n",
    "    # Configuración para 'elasticnet'\n",
    "    {\n",
    "        'penalty': ['elasticnet'],\n",
    "        'solver': ['saga'],  # Solo 'saga' es compatible con elasticnet\n",
    "        'C': [0.1, 1, 10],\n",
    "        'l1_ratio': [0.5],\n",
    "        'tol': [0.0001, 0.001],\n",
    "        'max_iter': [100, 200],\n",
    "        'class_weight': [None, 'balanced'],\n",
    "        'fit_intercept': [True],\n",
    "    },\n",
    "    # Configuración para 'l1' y 'l2'\n",
    "    {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga'],  # Solvers compatibles con l1 y l2\n",
    "        'C': [0.1, 1, 10],\n",
    "        'tol': [0.0001, 0.001],\n",
    "        'max_iter': [100, 200],\n",
    "        'class_weight': [None, 'balanced'],\n",
    "        'fit_intercept': [True],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Configurar el modelo de regresión logística\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Usar GridSearchCV para buscar la mejor combinación de parámetros\n",
    "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores parámetros y el mejor puntaje\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Mejores parámetros:\", best_params)\n",
    "print(\"Mejor puntuación de validación cruzada:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutamos para el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de Confusión:\n",
      "[[27575   857]\n",
      " [   14    35]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Correctas       1.00      0.97      0.98     28432\n",
      "Fraudulentas       0.04      0.71      0.07        49\n",
      "\n",
      "    accuracy                           0.97     28481\n",
      "   macro avg       0.52      0.84      0.53     28481\n",
      "weighted avg       1.00      0.97      0.98     28481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(x_reduced)\n",
    "test_accuracy = accuracy_score(y_test_total, y_pred)\n",
    "\n",
    "# Calcular la matriz de confusión y el reporte de clasificación\n",
    "conf_matrix = confusion_matrix(y_test_total, y_pred)\n",
    "report = classification_report(y_test_total, y_pred, target_names=['Correctas', 'Fraudulentas'])\n",
    "\n",
    "# Mostrar la matriz de confusión y el reporte\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regresion con mrMr y GLVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No se van a usar los acercamientos con GLVQ dada la imposibilidad con el entrenamiento del modelo de regresion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regresion con RFC clusterCentroid_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n",
      "Mejores parámetros: {'C': 1, 'class_weight': None, 'fit_intercept': True, 'l1_ratio': 0.5, 'max_iter': 200, 'penalty': 'elasticnet', 'solver': 'saga', 'tol': 0.0001}\n",
      "Mejor puntuación de validación cruzada: 0.925162321446409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joses\\miniconda3\\envs\\pre_procesamiento\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df_reduce_RFC_instances = datasets[\"reduce_RFC_instances\"]\n",
    "X = df_reduce_RFC_instances.drop(columns=['Class'])\n",
    "y = df_reduce_RFC_instances['Class']\n",
    "\n",
    "# Dividir en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = [\n",
    "    # Configuración para 'elasticnet'\n",
    "    {\n",
    "        'penalty': ['elasticnet'],\n",
    "        'solver': ['saga'],  # Solo 'saga' es compatible con elasticnet\n",
    "        'C': [0.1, 1, 10],\n",
    "        'l1_ratio': [0.5],\n",
    "        'tol': [0.0001, 0.001],\n",
    "        'max_iter': [100, 200],\n",
    "        'class_weight': [None, 'balanced'],\n",
    "        'fit_intercept': [True],\n",
    "    },\n",
    "    # Configuración para 'l1' y 'l2'\n",
    "    {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga'],  # Solvers compatibles con l1 y l2\n",
    "        'C': [0.1, 1, 10],\n",
    "        'tol': [0.0001, 0.001],\n",
    "        'max_iter': [100, 200],\n",
    "        'class_weight': [None, 'balanced'],\n",
    "        'fit_intercept': [True],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Configurar el modelo de regresión logística\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Usar GridSearchCV para buscar la mejor combinación de parámetros\n",
    "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores parámetros y el mejor puntaje\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Mejores parámetros:\", best_params)\n",
    "print(\"Mejor puntuación de validación cruzada:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutamos para el conjunto prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "0    28432\n",
      "1       49\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Cogemos los datos de test y les eliminamos las cracterísticas que no necesitamos\n",
    "X = data_prueba.drop(columns=['Class'])\n",
    "y_test_final = data_prueba['Class']\n",
    "### Version con RandomForestClassifier\n",
    "columns_to_keep_RFC = ['V17', 'V16', 'V12', 'V14', 'V11', 'V10', 'V9', 'V4', 'V18', 'V7']\n",
    "X_reduce_RFC = X[columns_to_keep_RFC]\n",
    "\n",
    "print(y_test_final.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de Confusión:\n",
      "[[28133   299]\n",
      " [    5    44]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Correctas       1.00      0.99      0.99     28432\n",
      "Fraudulentas       0.13      0.90      0.22        49\n",
      "\n",
      "    accuracy                           0.99     28481\n",
      "   macro avg       0.56      0.94      0.61     28481\n",
      "weighted avg       1.00      0.99      0.99     28481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_reduce_RFC)\n",
    "test_accuracy = accuracy_score(y_test_total, y_pred)\n",
    "\n",
    "# Calcular la matriz de confusión y el reporte de clasificación\n",
    "conf_matrix = confusion_matrix(y_test_total, y_pred)\n",
    "report = classification_report(y_test_total, y_pred, target_names=['Correctas', 'Fraudulentas'])\n",
    "\n",
    "# Mostrar la matriz de confusión y el reporte\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regresion con RFC clusterCentroid_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n",
      "Mejores parámetros: {'C': 10, 'class_weight': 'balanced', 'fit_intercept': True, 'l1_ratio': 0.5, 'max_iter': 200, 'penalty': 'elasticnet', 'solver': 'saga', 'tol': 0.0001}\n",
      "Mejor puntuación de validación cruzada: 0.925162321446409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joses\\miniconda3\\envs\\pre_procesamiento\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df_reduce_RFC_instances_hard = datasets[\"reduce_RFC_instances_hard\"]\n",
    "X = df_reduce_RFC_instances_hard.drop(columns=['Class'])\n",
    "y = df_reduce_RFC_instances_hard['Class']\n",
    "\n",
    "# Dividir en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = [\n",
    "    # Configuración para 'elasticnet'\n",
    "    {\n",
    "        'penalty': ['elasticnet'],\n",
    "        'solver': ['saga'],  # Solo 'saga' es compatible con elasticnet\n",
    "        'C': [0.1, 1, 10],\n",
    "        'l1_ratio': [0.5],\n",
    "        'tol': [0.0001, 0.001],\n",
    "        'max_iter': [100, 200],\n",
    "        'class_weight': [None, 'balanced'],\n",
    "        'fit_intercept': [True],\n",
    "    },\n",
    "    # Configuración para 'l1' y 'l2'\n",
    "    {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga'],  # Solvers compatibles con l1 y l2\n",
    "        'C': [0.1, 1, 10],\n",
    "        'tol': [0.0001, 0.001],\n",
    "        'max_iter': [100, 200],\n",
    "        'class_weight': [None, 'balanced'],\n",
    "        'fit_intercept': [True],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Configurar el modelo de regresión logística\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Usar GridSearchCV para buscar la mejor combinación de parámetros\n",
    "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores parámetros y el mejor puntaje\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Mejores parámetros:\", best_params)\n",
    "print(\"Mejor puntuación de validación cruzada:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutamos para el conjunto prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de Confusión:\n",
      "[[28120   312]\n",
      " [    5    44]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Correctas       1.00      0.99      0.99     28432\n",
      "Fraudulentas       0.12      0.90      0.22        49\n",
      "\n",
      "    accuracy                           0.99     28481\n",
      "   macro avg       0.56      0.94      0.61     28481\n",
      "weighted avg       1.00      0.99      0.99     28481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_reduce_RFC)\n",
    "test_accuracy = accuracy_score(y_test_total, y_pred)\n",
    "\n",
    "# Calcular la matriz de confusión y el reporte de clasificación\n",
    "conf_matrix = confusion_matrix(y_test_total, y_pred)\n",
    "report = classification_report(y_test_total, y_pred, target_names=['Correctas', 'Fraudulentas'])\n",
    "\n",
    "# Mostrar la matriz de confusión y el reporte\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regresion con RFC y GLVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No se va a usar el acercamiento con GLVQ dada la imposibilidad con el entrenamiento del modelo de regresion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión logística - Elección mejor preprocesamiento\n",
    "\n",
    "|                        | Precisión train | Precisión test | f1 test |\n",
    "|------------------------|-----------------|----------------|---------|\n",
    "| **mrMr - CC soft**     | **0.84**       | **0.52**       | **0.53**|\n",
    "| **mrMr - CC hard**     | **0.85**       | **0.52**       | **0.53**|\n",
    "| **RFC - CC soft**      | **0.93**       | **0.56**       | **0.61**|\n",
    "| **RFC - CC hard**      | **0.93**       | **0.56**       | **0.61**|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La combinación de modelos y estrategias de preprocesamiento muestra variaciones claras en las métricas de precisión y el comportamiento de los modelos frente a datos desbalanceados.\n",
    "Los métodos \"hard\" tienden a favorecer la mejora en las predicciones para la clase minoritaria (fraudulentas), a costa de una ligera pérdida en el rendimiento de la clase mayoritaria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de Confusión y Clasificación:\n",
    "\n",
    "### mrMr - CC soft:\n",
    "\n",
    "Muestra una precisión muy alta para la clase mayoritaria (correctas), pero un rendimiento pobre para la clase minoritaria (fraudulentas).\n",
    "Aunque tiene una tasa de recall del 71% en la clase minoritaria, la precisión del 4% implica un alto número de falsos positivos.\n",
    "Este modelo destaca más en mantener la precisión para las predicciones generales que en equilibrar clases.\n",
    "\n",
    "### mrMr - CC hard:\n",
    "\n",
    "Similar a la versión \"soft\", pero con mayor recall en fraudulentas debido al ajuste del preprocesamiento. Esto refuerza las detecciones para la clase minoritaria, aunque no mejora significativamente la precisión.\n",
    "\n",
    "### RFC - CC soft:\n",
    "\n",
    "Muy similar al modelo anterior (mrMr soft) en términos de matriz de confusión y métricas. La precisión sigue siendo baja en la clase minoritaria.\n",
    "Aunque los resultados son consistentes en la clase mayoritaria, las predicciones para fraudulentas tienen pocas mejoras.\n",
    "\n",
    "### RFC - CC hard:\n",
    "\n",
    "Este modelo logra el mejor equilibrio entre clases, con un recall del 90% y una precisión mejorada al 12% para la clase minoritaria. Aunque sigue siendo baja, representa un avance considerable comparado con las otras configuraciones.\n",
    "El f1-score de 0.61 en general refleja su capacidad para detectar correctamente ambas clases.\n",
    "\n",
    "### Conclusiones:\n",
    "El desbalance de clases (con una clase minoritaria que representa solo el 0.17% del total) es un desafío importante para los modelos. Los métodos de preprocesamiento \"hard\" parecen abordar mejor este problema.\n",
    "\n",
    "Las matrices de confusión muestran que los modelos \"soft\" generan un número alto de falsos negativos para la clase minoritaria (fraudulentas), mientras que los modelos \"hard\" logran reducirlos significativamente.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pre_procesamiento",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
